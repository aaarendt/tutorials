{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are a type of machine learning model trained to learn a probability distribution over words. They've been used for various applications, including text generation, question answering, text summarization, language translation, and speech recognition. \n",
    "\n",
    "These models have existed since the 1980s and are mainly categorized into two kinds: (1) **statistical models** that use statistical techniques such as N-grams, hidden markov models, etc., and (2) **neural models** that use neural networks for training.\n",
    "\n",
    "Humans are inherently good at learning the probability of the next word. For example, if asked which of the below sentences has a higher probability for you to encounter, we know that the probability of the first sentence is greater than the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  p(\\text{jupiter is the largest planet}) > p(\\text{jupiter is the largest moon})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling aims to train models that can do well in tasks like the above statement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An N-gram is a sequence of N words (or tokens.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the sentence : `The Sun is the Solar System's star and by far its most massive component. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigram, or 1-gram for the above sentence would be: `\"The\", \"Sun\", \"is\", \"the\", \"Solar\", \"Systems\", \"star\", \"and\", \"by\", \"far\", \"its\", \"most\", \"massive\", \"component\"`\n",
    "\n",
    "A bigram, or 2-gram for the above sentence would be: `\"The Sun\", \"Sun is\", \"is the\", \"the Solar\", \"Solar Systems\", \"Systems star\", \"star and\", \"and by\", \"by far\", \"far its\", \"its most\", \"most massive\", \"massive component\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram models consider the probabiliyt of the given word, given the (N-1) previous words. For trigrams, they consider the probability of a word given the two previous words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and N-Gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demystify how language models in principle, we will **build an n-gram language model** from scratch in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
