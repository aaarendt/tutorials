{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb54058-bf40-4ecd-bb10-5eb3e6c5aa8b",
   "metadata": {},
   "source": [
    "# LLMs, Prompt Engineering, and OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75972fd4-1e17-40e2-865c-ad10e5caeace",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59c6a8-cddf-4308-9b7a-9d5b3f09edcb",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amounts of textual information available on the Internet, including books, articles, websites, and other digital content. Getting into the weeds of how these models are trained is out of the scope of this tutorial, but we have added links to papers and tutorials if you'd like to understand how LLMs are trained. Do note that training LLMs is expensive; the cost can easily increase to millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f76df-7a8d-4fe8-b983-86aa7783c286",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ce8a-4fde-495a-9dce-12cabfcd1501",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d95306-9a8f-45cb-b3ed-c02ed65b87eb",
   "metadata": {},
   "source": [
    "For this entire tutorial, we will be using [Open Language Model: OLMo](https://allenai.org/olmo), an open LLM framework built by [Allen Institute for AI](https://allenai.org/). With this open framework, you can access its complete pretraining data ([dolma](https://github.com/allenai/dolma)), training code, model weights, and evaluation suite. Tracking openness, transparency, accountability, and risks in LLMs is a growing research area. Checkout this [tool](https://opening-up-chatgpt.github.io/) to understand the range of openness in these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef1a9d-bde0-413a-87b6-d6e90b29324b",
   "metadata": {},
   "source": [
    "We have chosen a 7B instruction-tuned OLMo model that we have compressed to speed up its inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6c106f-9670-48a9-a0ab-3e4d292b3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama # Python bindings for llama.cpp, to enable LLM inference with minimal setup\n",
    "from ssec_tutorials.scipy_conf import * # Contains helper methods for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555af009-c315-46ae-b7be-5f4d4562e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043d0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from the huggingface hub: https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF\n",
    "# TODO: Change this and load the model locally\n",
    "olmo = Llama.from_pretrained(repo_id=\"ssec-uw/OLMo-7B-Instruct-GGUF\", filename=\"OLMo-7B-Instruct-Q4_K_M.gguf\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875930-23c6-4d7b-be5b-70fabc3cfbe5",
   "metadata": {},
   "source": [
    "Note the `7B,` `Instruct,` `GGUF,` and `Q4_K_M` keywords here.\n",
    "\n",
    "**7B**: B stands for billion, and 7B suggests that this specific model has 7 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1a4fc-5205-466f-b534-d766fa2e2509",
   "metadata": {},
   "source": [
    "**Base models**, for example [AllenAi's OLMo-7B](https://huggingface.co/allenai/OLMo-7B), [AllenAi's OLMo-1B](https://huggingface.co/allenai/OLMo-1B), and [Meta's Llama-3-8B](meta-llama/Meta-Llama-3-8B) processes billions of words and texts. The training process is semi-supervised, meaning data is supplied without much annotation or labeling, but much effort is poured into improving the data quality. We have found that training the model with tremendous amount of text allows it to learn language patterns and general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dbe54-4f2a-4720-89d0-66a36490d707",
   "metadata": {},
   "source": [
    "When prompted, the model predicts the next tokens (words) statistically likely to follow.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87fc4ce-176f-4f3a-8bf8-565934d772f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"Jupiter is the largest\", echo=True, max_tokens=1, temperature=0.8) # Generate a completion, can also call olmo.create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368d12c1-2994-479a-ac41-6c68328add24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the largest planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3803f08-a1d4-49bd-b4db-1d5cdea70e6c",
   "metadata": {},
   "source": [
    "But when prompted with, `What is the capital of Washington state in the USA?`, a base model **could** generate logical text that may or may not contain the right answer. \n",
    "\n",
    "This is when `Instruction` fine-tuning comes into play, which enhances the base model's ability to execute specific tasks. For `Instruction` fine-tuning, we can take the base models and further train them on much smaller and more specific datasets. For this tutorial, we are using a **quantized**, in other words **compressed** model version of [OLMo-7B-Instruct](https://huggingface.co/allenai/OLMo-7B-Instruct), which has been fine-tuned on [UltraFeedback Dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized). That is where the keyword **Instruct** comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92e22e-3e6d-428f-af5a-98671dcf967b",
   "metadata": {},
   "source": [
    "`GGUF` is a file format for storing models for inference with GGML and executors based on GGML, a tensor library for machine learning. \n",
    "\n",
    "**Quantization** reduces a high-precision representation (usually the regular 32-bit floating-point) for weights and activations to a lower-precision data type, in `Q4_K_M` each weight is reduced to a 4-bit representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327c0bbe-1d83-4a4f-88a4-df19e1973ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"What is the capital of Washington state in the USA?\", echo=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d32981-854d-4d0a-91a9-259850609995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Washington state in the USA?\n",
      "Olympia is the capital of the U.S. state of Washington.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588714ce-5b4f-4f81-ac56-fd29c765e719",
   "metadata": {},
   "source": [
    "## LLM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e438756-3ce3-480b-a6f8-2b6dde618893",
   "metadata": {},
   "source": [
    "We typically interact with the LLM via an API through which we can send prompts, and we can configure different parameters to get different results from LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9df1d8-1fc0-4c32-acca-e75f367d57d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'prompt': <Parameter \"prompt: 'str'\">,\n",
       "              'suffix': <Parameter \"suffix: 'Optional[str]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = 16\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.8\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[int]' = None\">,\n",
       "              'echo': <Parameter \"echo: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'stopping_criteria': <Parameter \"stopping_criteria: 'Optional[StoppingCriteriaList]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(olmo).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98c2c3-5921-4a59-aec7-434b905e261b",
   "metadata": {},
   "source": [
    "Some standard parameters are:\n",
    "\n",
    "**prompt:** The prompt to generate text from.\n",
    "\n",
    "**max_tokens:** The maximum number of tokens to generate.\n",
    "\n",
    "**temperature:** A higher temperature produces more creative and diverse output, while a lower temperature produces more deterministic output. In practical terms, you should use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For creative tasks, it might be beneficial to increase the temperature value.\n",
    "\n",
    "**top_p:** This parameter, in conjunction with temperature, offers a powerful tool for controlling the model's output. Known as nucleus sampling, it allows you to determine the level of determinism in the responses. By using `top_p`, you can specify that only the tokens comprising the top_p probability mass are considered for responses. A low top_p value selects the most confident responses, while a higher value prompts the model to consider more possible words, leading to more diverse outputs. The general recommendation is to alter `temperature` or `top_p` but not both.\n",
    "\n",
    "**stop:** A list of strings to stop generation when encountered. This is another way to control the length and structure of the model's response. \n",
    "\n",
    "**frequency_penalty:** The frequency penalty applies a penalty on the next token based on how many times that token has already appeared in the generated response and prompt. The higher the frequency penalty, the less likely a word will reappear. This setting reduces the repetition of words in the generated response by giving tokens that appear more a higher penalty.\n",
    "\n",
    "**presence_penalty:** The presence penalty applies the same penalty for all repeated tokens. A token that appears twice and a token that appears n times are penalized the same. You may choose a higher presence penalty if you want the model to generate diverse or creative text. \n",
    "\n",
    "To learn more about other parameters, refer to [create_completion API reference.](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.create_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89175b91-30a3-40c9-aa97-0345c7249900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"Write a sarcastic but nice poem about the city of Seattle\", echo=True, temperature=1, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68867647-0f7b-45f0-9ad4-f2dfc886e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a sarcastic but nice poem about the city of Seattle.\n",
      "\n",
      "Title: \"A Slight Angle on the Emerald City\"\n",
      "\n",
      "The city of Seattle, with its rainy days and its grays\n",
      "Where coffee shops are the norm, and its hipsters are quite the sight\n",
      "It's not exactly the Sunset, but it is a place to be seen\n",
      "With its rainforest-like parks and its chai teas, too\n",
      "\n",
      "But alas, this city of ours, it's not all that\n",
      "The hipster fashion, though charming, can get old fast\n",
      "And while we may enjoy our drizzly afternoons in the park\n",
      "Our coffee culture is still something to be desired.\n",
      "\n",
      "Seattle, you see, are a bunch of grumpy-faced natives\n",
      "With moody skies above and a love for that good brew\n",
      "But deep down, there's more to this city than meets the eye\n",
      "A little warmth, some sunshine, would make it all right.\n",
      "\n",
      "So if you're ever in Seattle, be prepared for rain\n",
      "And embrace the wet weather with open arms\n",
      "For despite its quirks and its flaws, this city is worth a visit or two.\n",
      "With its artsy scene and its quirky charm, there's no doubt it stands out.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725a982-2e97-4c9e-bdb4-7176a31a2873",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae666e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an astro physics expert that answers questions around astrophysics.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is dark matter?\"},\n",
    "    ],\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aec84c1-db33-498c-9641-6b9a0670aae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'messages': <Parameter \"messages: 'List[ChatCompletionRequestMessage]'\">,\n",
       "              'functions': <Parameter \"functions: 'Optional[List[ChatCompletionFunction]]' = None\">,\n",
       "              'function_call': <Parameter \"function_call: 'Optional[ChatCompletionRequestFunctionCall]' = None\">,\n",
       "              'tools': <Parameter \"tools: 'Optional[List[ChatCompletionTool]]' = None\">,\n",
       "              'tool_choice': <Parameter \"tool_choice: 'Optional[ChatCompletionToolChoiceOption]' = None\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.2\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'response_format': <Parameter \"response_format: 'Optional[ChatCompletionRequestResponseFormat]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = None\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[bool]' = None\">,\n",
       "              'top_logprobs': <Parameter \"top_logprobs: 'Optional[int]' = None\">})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(olmo.create_chat_completion).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b54c9-f225-461f-a7d2-18315a667052",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. https://news.ycombinator.com/item?id=35712334\n",
    "2. https://benjaminwarner.dev/2023/07/01/attention-mechanism\n",
    "3. [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://dl.acm.org/doi/10.1145/3571884.3604316)\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37861013-d365-48a6-8732-ae2006fb864e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
