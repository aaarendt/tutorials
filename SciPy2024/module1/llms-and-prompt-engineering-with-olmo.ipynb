{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb54058-bf40-4ecd-bb10-5eb3e6c5aa8b",
   "metadata": {},
   "source": [
    "# LLMs, Prompt Engineering, and OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75972fd4-1e17-40e2-865c-ad10e5caeace",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59c6a8-cddf-4308-9b7a-9d5b3f09edcb",
   "metadata": {},
   "source": [
    "An introduction to large language models and how they're trained is out of scope, but they have been trained over large amounts of textual information available on the Internet, including books, articles, websites, and other digital content. Getting into the weeds of how these models are trained is out of the scope of this tutorial, but we have added links to papers and tutorials if you'd like to understand how LLMs are trained. Do note that training LLMs is expensive; the cost can easily increase to millions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f76df-7a8d-4fe8-b983-86aa7783c286",
   "metadata": {},
   "source": [
    "Early language models could predict the probability of a single word token or n-grams; modern large language models can predict the likelihood of sentences, paragraphs, or entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4ce8a-4fde-495a-9dce-12cabfcd1501",
   "metadata": {},
   "source": [
    "However, LLMs are notoriously unable to retrieve and manipulate the knowledge they possess, which leads to issues like hallucination (i.e., generating factually incorrect information), knowledge cutoffs, and poor performance in domain-specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d95306-9a8f-45cb-b3ed-c02ed65b87eb",
   "metadata": {},
   "source": [
    "For this entire tutorial, we will be using [Open Language Model: OLMo](https://allenai.org/olmo), an open LLM framework built by [Allen Institute for AI](https://allenai.org/). With this open framework, you can access its complete pretraining data ([dolma](https://github.com/allenai/dolma)), training code, model weights, and evaluation suite. Tracking openness, transparency, accountability, and risks in LLMs is a growing research area. Checkout this [tool](https://opening-up-chatgpt.github.io/) to understand the range of openness in these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef1a9d-bde0-413a-87b6-d6e90b29324b",
   "metadata": {},
   "source": [
    "We have chosen a 7B instruction-tuned OLMo model that we have compressed to speed up its inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6c106f-9670-48a9-a0ab-3e4d292b3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama # Python bindings for llama.cpp, to enable LLM inference with minimal setup\n",
    "from ssec_tutorials.scipy_conf import * # Contains helper methods for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555af009-c315-46ae-b7be-5f4d4562e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043d0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model from huggingface hub: https://huggingface.co/ssec-uw/OLMo-7B-Instruct-GGUF\n",
    "olmo = Llama.from_pretrained(repo_id=\"ssec-uw/OLMo-7B-Instruct-GGUF\", filename=\"OLMo-7B-Instruct-Q4_K_M.gguf\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875930-23c6-4d7b-be5b-70fabc3cfbe5",
   "metadata": {},
   "source": [
    "Note the `7B,` `Instruct,` `GGUF,` and `Q4_K_M` keywords here.\n",
    "\n",
    "**7B**: B stands for billion, and 7B suggests that this specific model has 7 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1a4fc-5205-466f-b534-d766fa2e2509",
   "metadata": {},
   "source": [
    "Base models, for example [AllenAi's OLMo-7B](https://huggingface.co/allenai/OLMo-7B), [AllenAi's OLMo-1B](https://huggingface.co/allenai/OLMo-1B), and [Meta's Llama-3-8B](meta-llama/Meta-Llama-3-8B) processes billions of words and texts. The training process is semi-supervised, meaning data is supplied without much annotation or labeling, but much effort is poured into improving the data quality. We have found that training the model with tremendous amount of text allows it to learn language patterns and general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dbe54-4f2a-4720-89d0-66a36490d707",
   "metadata": {},
   "source": [
    "When prompted, the model predicts the next tokens (words) statistically likely to follow.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87fc4ce-176f-4f3a-8bf8-565934d772f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"Jupiter is the largest\", echo=True, max_tokens=1, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368d12c1-2994-479a-ac41-6c68328add24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the largest planet\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3803f08-a1d4-49bd-b4db-1d5cdea70e6c",
   "metadata": {},
   "source": [
    "But when prompted with, `What is the capital of Washington state in the USA?`, a base model **could** generate logical text that may or may not contain the right answer. This is when `Instruction` fine-tuning comes into play, which enhances the base model's ability to execute specific tasks. \n",
    "\n",
    "\n",
    "> Add more information about how instruct models are trained and chat templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327c0bbe-1d83-4a4f-88a4-df19e1973ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = olmo(prompt=\"What is the capital of Washington state in the USA?\", echo=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d32981-854d-4d0a-91a9-259850609995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Washington state in the USA?\n",
      "Olympia is the capital of the U.S. state of Washington.\n"
     ]
    }
   ],
   "source": [
    "print(parse_text_generation_response(model_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e147d2-df99-42f3-a90a-d576ac428f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9df1d8-1fc0-4c32-acca-e75f367d57d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'prompt': <Parameter \"prompt: 'str'\">,\n",
       "              'suffix': <Parameter \"suffix: 'Optional[str]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = 16\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.8\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[int]' = None\">,\n",
       "              'echo': <Parameter \"echo: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'stopping_criteria': <Parameter \"stopping_criteria: 'Optional[StoppingCriteriaList]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(olmo).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae666e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = olmo.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an astro physics expert that answers questions around astrophysics.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is dark matter?\"},\n",
    "    ],\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aec84c1-db33-498c-9641-6b9a0670aae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'messages': <Parameter \"messages: 'List[ChatCompletionRequestMessage]'\">,\n",
       "              'functions': <Parameter \"functions: 'Optional[List[ChatCompletionFunction]]' = None\">,\n",
       "              'function_call': <Parameter \"function_call: 'Optional[ChatCompletionRequestFunctionCall]' = None\">,\n",
       "              'tools': <Parameter \"tools: 'Optional[List[ChatCompletionTool]]' = None\">,\n",
       "              'tool_choice': <Parameter \"tool_choice: 'Optional[ChatCompletionToolChoiceOption]' = None\">,\n",
       "              'temperature': <Parameter \"temperature: 'float' = 0.2\">,\n",
       "              'top_p': <Parameter \"top_p: 'float' = 0.95\">,\n",
       "              'top_k': <Parameter \"top_k: 'int' = 40\">,\n",
       "              'min_p': <Parameter \"min_p: 'float' = 0.05\">,\n",
       "              'typical_p': <Parameter \"typical_p: 'float' = 1.0\">,\n",
       "              'stream': <Parameter \"stream: 'bool' = False\">,\n",
       "              'stop': <Parameter \"stop: 'Optional[Union[str, List[str]]]' = []\">,\n",
       "              'seed': <Parameter \"seed: 'Optional[int]' = None\">,\n",
       "              'response_format': <Parameter \"response_format: 'Optional[ChatCompletionRequestResponseFormat]' = None\">,\n",
       "              'max_tokens': <Parameter \"max_tokens: 'Optional[int]' = None\">,\n",
       "              'presence_penalty': <Parameter \"presence_penalty: 'float' = 0.0\">,\n",
       "              'frequency_penalty': <Parameter \"frequency_penalty: 'float' = 0.0\">,\n",
       "              'repeat_penalty': <Parameter \"repeat_penalty: 'float' = 1.1\">,\n",
       "              'tfs_z': <Parameter \"tfs_z: 'float' = 1.0\">,\n",
       "              'mirostat_mode': <Parameter \"mirostat_mode: 'int' = 0\">,\n",
       "              'mirostat_tau': <Parameter \"mirostat_tau: 'float' = 5.0\">,\n",
       "              'mirostat_eta': <Parameter \"mirostat_eta: 'float' = 0.1\">,\n",
       "              'model': <Parameter \"model: 'Optional[str]' = None\">,\n",
       "              'logits_processor': <Parameter \"logits_processor: 'Optional[LogitsProcessorList]' = None\">,\n",
       "              'grammar': <Parameter \"grammar: 'Optional[LlamaGrammar]' = None\">,\n",
       "              'logit_bias': <Parameter \"logit_bias: 'Optional[Dict[str, float]]' = None\">,\n",
       "              'logprobs': <Parameter \"logprobs: 'Optional[bool]' = None\">,\n",
       "              'top_logprobs': <Parameter \"top_logprobs: 'Optional[int]' = None\">})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(olmo.create_chat_completion).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b54c9-f225-461f-a7d2-18315a667052",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. https://news.ycombinator.com/item?id=35712334\n",
    "2. https://benjaminwarner.dev/2023/07/01/attention-mechanism\n",
    "3. [Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](https://dl.acm.org/doi/10.1145/3571884.3604316)\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37861013-d365-48a6-8732-ae2006fb864e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
