{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6533bcd-e294-4fbe-b8d5-6df3053ccc04",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fde1b-f2a4-4c57-8a94-c3dd58a10d74",
   "metadata": {},
   "source": [
    "Although trained on large datasets, stale data can severely limit LLMs. It faces several challenges:\n",
    "\n",
    "1. The models are trained on internet content, so they might not generate relevant output when prompted for information that is not publicly available on the internet.\n",
    "\n",
    "2. The models are trained up to a certain date, they might not generate relevant output when prompted for content and information that has happened after the training completion date of the model.\n",
    "\n",
    "3. The models are trained to be more generalized. This means that they can only produce generic outputs and might not perform as expected when prompted for specific deep-dive concepts related to a particular topic.\n",
    "\n",
    "One way to dynamically integrate relevant external information is retrieval-augmented generation (RAG), which can help improve the reliability of LLM outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8f06f-b377-4280-9a35-0d2c832904d4",
   "metadata": {},
   "source": [
    "## RAG Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb892294-f03e-4ad5-9e7b-269526de7a36",
   "metadata": {},
   "source": [
    "RAG proposes a solution to this issue by supplementing the prompt sent to the LLM with information from external sources through a retrieval model, thereby providing the LLM with more relevant input to generation responses. It allows you to use pre-trained LLMs without fine-tuning them or training your own LLM on your training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85866b9-4dac-46a3-8803-b2a71d8dc7d4",
   "metadata": {},
   "source": [
    "> TODO: Add RAG image here with citation. Use [Mermaid](https://mermaid.live/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430de73-e46c-4029-a460-dbc65184208c",
   "metadata": {},
   "source": [
    "Multiple concepts influence RAG pipeline:\n",
    "\n",
    "1. Pre-retrieval\n",
    "2. Retrieval\n",
    "3. Augmentation\n",
    "4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855b786-5d33-4e4c-b58c-ab8c039deae8",
   "metadata": {},
   "source": [
    "## Pre-Retrieval\n",
    "\n",
    "The pre-retrieval phase can also be considered the data and query/prompt preparation phase, focusing on efficient information retrieval or data access. To improve your RAG pipeline, the pre-retrieval phase contains tasks such as: `(1): Indexing, (2) Query Manipulation, and (3) Data Modification.` In this tutorial, we primarily focus on indexing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abe984-c0d4-4b22-b74e-f3b00bbe0260",
   "metadata": {},
   "source": [
    "`Indexing` enables fast and accurate information retrieval that sets up the context for any LLM to improve its response to a given user prompt or query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb0cdf-ac35-4546-90c5-fd8082633ca5",
   "metadata": {},
   "source": [
    "We will be indexing abstracts for all astrophysics papers and Astropy's documentation, a common core package for Astronomy in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "65456be6-0b21-4580-9413-03f5829a8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d2709c7b-e9d2-476a-92f9-63df3f91ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the already pickled file but refer to the notebook in the Appendix if you are interested in understanding how we built the dataset\n",
    "astro_df = pd.read_pickle(\"../../resources/data/astro-ph-arXiv-abstracts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a8dbe4ff-a7f8-4ced-af2a-53455996ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of astrophysics papers:  331564\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of astrophysics papers: \", len(astro_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "05afabec-87a7-423a-8acf-de99c77ebf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712.2086</td>\n",
       "      <td>On weak and strong magnetohydrodynamic turbulence</td>\n",
       "      <td>Recent numerical and observational studies c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>712.2103</td>\n",
       "      <td>Hilltop Curvatons</td>\n",
       "      <td>We study ``hilltop'' curvatons that evolve o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>712.211</td>\n",
       "      <td>Near-field cosmology with the VLT</td>\n",
       "      <td>With the arrival of wide-field imagers on me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>712.2111</td>\n",
       "      <td>The prototype colliding-wind pinwheel WR 104</td>\n",
       "      <td>Results from the most extensive study of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>712.2116</td>\n",
       "      <td>X-ray spectral evolution of TeV BL Lac objects...</td>\n",
       "      <td>Many of the extragalactic sources detected i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  712.2086  On weak and strong magnetohydrodynamic turbulence   \n",
       "1  712.2103                                  Hilltop Curvatons   \n",
       "2   712.211                  Near-field cosmology with the VLT   \n",
       "3  712.2111       The prototype colliding-wind pinwheel WR 104   \n",
       "4  712.2116  X-ray spectral evolution of TeV BL Lac objects...   \n",
       "\n",
       "                                            abstract  \n",
       "0    Recent numerical and observational studies c...  \n",
       "1    We study ``hilltop'' curvatons that evolve o...  \n",
       "2    With the arrival of wide-field imagers on me...  \n",
       "3    Results from the most extensive study of the...  \n",
       "4    Many of the extragalactic sources detected i...  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "65044f39-b8f1-453a-b5e2-a69a7d31ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b11283-1817-4921-9863-d1e08e848334",
   "metadata": {},
   "source": [
    "### Documents Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a5c19-5b19-4901-bd14-84bd4ee09399",
   "metadata": {},
   "source": [
    "LangChain helps load different documents (.txt, .pdf, .docx, .csv, .xlsx, .json) to feed into the LLM. The Document Loader even allows YouTube audio parsing and loading as part of unstructured document loading.\n",
    "\n",
    "Once loaded into the LangChain, the document can be pre-processed in different ways as required in the LLM application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "87e0c5ee-c8cc-44ba-b171-ef145224043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe full of abstracts\n",
    "# to memory in the form of LangChain Document objects\n",
    "loader = DataFrameLoader(astro_df, page_content_column=\"abstract\") \n",
    "astrophysics_abstracts_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "75b6f10c-4fc8-4d49-bb9d-ef5172c3288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of astrophysics papers:  331564\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of astrophysics papers: \", len(astrophysics_abstracts_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbcf1d-499d-4b82-ab23-205ee6d078d3",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86537e0-7c49-43c7-b985-f028b9036c46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Embeddings, also called \"Vector Embedding,\" help LLMs develop a semantic understanding of the textual data they are trained on. In simpler terms, these embedding models lay the groundwork for LLMs to perform tasks like sentence completion, similarity search, questions and answers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048945d-c67d-4ecd-ae07-57f694a5ec52",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab12955-0056-409d-b23a-af27a48dd29d",
   "metadata": {},
   "source": [
    "At the lowest level, machines only understand numeric values. For LLMs to work, natural language is converted into an array of numeric values before they are fed into the models. These arrays of numeric values are called \"Vector.\"\n",
    "\n",
    "An example of a vector: [2.5, 1.0, 3.3, 7.8]\n",
    "\n",
    "The above is an example of a vector of size 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4a00edab-f0e2-4ac4-b9a9-d64f1f0adc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: [2.5 1.7 3.3 7.8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([2.5, 1.7, 3.3, 7.8])\n",
    "print(f\"Vector: {vector}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad8341-6516-4a1a-aab7-ea14cb5a53cc",
   "metadata": {},
   "source": [
    "#### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec2773-b602-457c-a674-d70bf2971dea",
   "metadata": {},
   "source": [
    "We stated above that **\"texts are converted into an array of numeric values called vectors\"**.\n",
    "\n",
    "But depending on your use case, each word, sentence, paragraph, or entire document can be represented as a vector. \n",
    "\n",
    "Tokens are the smallest natural language units converted into a vector. It could be at the character level, sub-word level, word level, sentence level, paragraph level, or document level.\n",
    "\n",
    "Example: Consider the text below.\n",
    "\n",
    "`Earth is a planet of the solar system. There are 9 planets in the solar system. \n",
    "All planets revolve around the sun. Sun is a star.`\n",
    "\n",
    "\n",
    "Case 1.) **Tokenizing the entire paragraph into vector.**  \n",
    "Tokenization: The entire paragraph is a single token.   \n",
    "Vectorization: A single vector.  \n",
    "Sample Vector Representation: [3.1, 6.8, 5.4, 8.0, 7.1]\n",
    "\n",
    "Case 2.) **Tokenizing each sentence into vectors.**  \n",
    "Tokenization: One token for each sentence (total 4 tokens)  \n",
    "Vectorization: One vector for each sentence (total 4 vectors).   \n",
    "Sample Vector Representation: [[1.2, 2.3, 3.8, 7.9, 0.8], [2.5, 3.0, 8.2, 6.6, 4.1], [3.2, 6.5, 8.1, 9.3, 1.4], [1.1, 0.7, 7.2, 3.5, 8.5]]\n",
    "\n",
    "Case 3.) **Tokenizing each word in the paragraph into a vector. There are 26 words in the paragraph, ignoring punctuation. Each word gets converted into a vector.**  \n",
    "Tokenization: One token for each word in the paragraph (26 tokens)  \n",
    "Vectorization: One vector for each token (total 26 vectors).    \n",
    "Sample Vector Representation: [[2.1, 3.2, 4.1, 9.8, 7.0], [8.2, 4.2, 7.1, 3.8, 2.0].....total 26 such represenatations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97f481-33c8-4672-a16c-308e7a6f2dc9",
   "metadata": {},
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06b3dc-e387-417c-aec9-01044b5f6a56",
   "metadata": {},
   "source": [
    "Tokenizers are components responsible for converting large texts into tokens (tokenization). Different types of pre-trained tokenizers are available. You can even train your own tokenizers. But for the scope of this tutorial, we will use a pre-trained one. \n",
    "\n",
    "Generally, each tokenizer follows the following steps:\n",
    "\n",
    "1. Break down the original text into tokens. These tokens could again be at the character, sub-word, word, sentence, paragraph, or document levels.\n",
    "2. Assign a unique identifier to each of the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "84e2701b-d65f-46b4-a84e-940cfc89df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, here is how you can split a short sentence into chunks of text\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "da90bd59-5295-40c4-98c6-fe5c6bdb9d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earth is a', 'planet in', 'the solar', 'system.']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "text_splitter.split_text(text=\"Earth is a planet in the solar system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5445-cc35-4c34-acd8-35aace637dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2f6605b-d212-45c2-89b3-54cf3eba5303",
   "metadata": {},
   "source": [
    "[Learn more about how to split text into tokens in LangChain here.](https://python.langchain.com/v0.2/docs/how_to/split_by_token/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1176aa-6658-43d8-b3d5-c02ff4827690",
   "metadata": {},
   "source": [
    "#### Embedding Models\n",
    "\n",
    "A language model needs to understand how tokens are related to each other in the context of human language. To understand this semantic relationship, these tokens are converted into numerical vectors.\n",
    "\n",
    "Embedding Models are trained upon these tokens to develop an \"embedding space.\"\n",
    "\n",
    "- Before the training, the embedding model initializes an N-dimensional 'vector' corresponding to each 'token' with random values. (Value of N depends on the embedding model)\n",
    "  \n",
    "- During the embedding model training, the values for these vectors are updated across iterations. In this process, similar or related tokens are updated to have similarly valued vectors.\n",
    "  \n",
    "- After the training, the collection of all the 'vectors' corresponding to all the tokens is called the \"embedding space.\"\n",
    "\n",
    "- \"Embedding Space\" is an encoded representation of meanings of tokens and inter-token relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1cf20f86-19d8-4e16-98d7-97b69f8f889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "afb875a2-5daf-4647-9c13-52a5bd6929cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 256\n"
     ]
    }
   ],
   "source": [
    "# Get the value of the max sequence_length\n",
    "print(f\"Model's maximum sequence length: {SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3b5a5e70-60a5-4270-b58c-b2f69e724193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4e3beed7-0042-4142-9226-8371ee2ef38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings_model.embed_query(\"Earth is a planet in the solar system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fe1a2a99-c7a6-45c0-879d-e3ca7f69cc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension of vector\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e10c62ff-cde7-4ad8-b9ff-09f8e54a22e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.030922148376703262, 0.039250507950782776, 0.01346262451261282]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6a255-2c4a-4a6e-ad53-cd7865c2165e",
   "metadata": {},
   "source": [
    "In an embedding space, you can find how similar two vectors are using `dot product` or  using `cosine similarity.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e09296df-1f27-4c9e-ad66-0b8ac874f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.7257]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity:\", util.dot_score(query_result, embeddings_model.embed_query(\"Mars is a planet in the solar system.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "044ccda7-acfe-406d-a5a9-a9c30f6390cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.0174]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity:\", util.dot_score(query_result, embeddings_model.embed_query(\"Hello Tacoma.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c3047-ef63-46fa-9501-8a6834364cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076ba80-a8e4-44e5-a69d-9d5b5b2e84da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
